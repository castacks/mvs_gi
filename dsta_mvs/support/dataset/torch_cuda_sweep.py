
# System packages.
import numpy as np

# PyTorch.
import torch

# Utils.
from ...mvs_utils.debug import ( show_obj, show_sum, show_elements )
from ...mvs_utils import torch_meshgrid
from ...mvs_utils.ftensor import FTensor

class RayMaker_UEPanorama(object):
    def __init__(self, distance, long_range, lat_range, frame_name='rbf'):
        '''
        distance (Array): Array of distance value.
        long_range (2-element): Longitude range.
        lat_range (2-element): Latitude range.
        frame_name (str): The name of the frame.
        '''
        super().__init__()
    
        # self.num_cand     = num_cand
        # # Deliberately use [0, num_cand] to cover num_cand - 1.
        # self.inv_dist_idx = torch.linspace(0, num_cand, stops)
        # self.bf           = bf
        # self.eps          = eps
        self.dist       = torch.from_numpy(distance)
        self.long_range = long_range
        self.lat_range  = lat_range
        self.frame_name = frame_name

    def make_rays_from_distance( self, dist ):
        '''
        This function uses the panorama frame, where z-axis is backward, x to the left, y downward.
         
        dist (Tensor): [ 1, H, W ], distance values.
        output (Tensor): [ 3, 1, H, W ]. The second dimension is for compatibility with the rays generated by candidates.
        '''

        H = dist.shape[-2] # To be compatible with dist that has More than 2 dimensions.
        W = dist.shape[-1]

        # Prepare phi and theta values.
        # phi   = torch.arange(0, H, device=dist.device) / ( H - 1 ) * \
        #             (self.lat_range[1] - self.lat_range[0]) + self.lat_range[0]
        # After changing the pixel coordinate definition.
        phi   = ( torch.arange(0, H, device=dist.device) + 0.5 ) / H * \
                    (self.lat_range[1] - self.lat_range[0]) + self.lat_range[0]

        # theta = torch.arange(0, W, device=dist.device) / ( W - 1 ) * \
        #             (self.long_range[1] - self.long_range[0]) + self.long_range[0] # NOTE: Use full range.
        # After changing the pixel coordinate definition.
        theta = ( torch.arange(0, W, device=dist.device) + 0.5 ) / W  * \
                    (self.long_range[1] - self.long_range[0]) + self.long_range[0]

        # Meshgrid.
        grid_phi, grid_theta = torch_meshgrid( phi, theta, indexing='ij' )

        # Intermediate values.
        sin_phi   = torch.sin( grid_phi )
        cos_phi   = torch.cos( grid_phi )
        sin_theta = torch.sin( grid_theta )
        cos_theta = torch.cos( grid_theta )

        ds =  dist * sin_phi
        x  =  ds   * cos_theta
        y  = -dist * cos_phi
        z  = -ds   * sin_theta

        # [ 3, H, W ] -> [ 3, 1, H, W ]
        # t = torch.stack( (x, y, z), dim=0 ).unsqueeze(1)
        t = torch.cat( (x, y, z), dim=0 ).unsqueeze(1)
        return FTensor(t, f0=self.frame_name)

    def make_rays_for_candidates(self, grid_shape):
        '''
        This function uses the panorama frame, where z-axis is backward, x to the left, y downward.
        
        grid_shape (2-element): [ H, W ]
        output (Tensor): [ 3, N, H, W ]
        '''
        
        # TODO: standarize the way we use the spherical coordinates.
        # Related research works are using a different definition.

        H, W = grid_shape

        # Prepare phi and theta values.
        # phi   = ( torch.arange(0, H) / ( H - 1 ) * (self.lat_range[1] - self.lat_range[0]) ) \
        #          + self.lat_range[0] #NOTE: Use [0,pi] for full range, including pi.
        # After changing the pixel coordinate definition.
        phi   = ( ( torch.arange(0, H) + 0.5 ) / H * (self.lat_range[1] - self.lat_range[0]) ) \
                 + self.lat_range[0]

        #Orignal scaling eq was * (2 * np.pi) - np.pi for a range of [-np.pi, np.pi)
        # theta = ( torch.arange(0, W) / ( W - 1 ) * (self.long_range[1] - self.long_range[0]) ) \
        #          + self.long_range[0] # NOTE: Use [-pi,pi] for full range, including pi.
        # After changing the pixel coordinate definition.
        theta = ( ( torch.arange(0, W) + 0.5 ) / W * (self.long_range[1] - self.long_range[0]) ) \
                 + self.long_range[0]

        show_obj(lat_range=self.lat_range, long_range=self.long_range, H=H, W=W)
        show_sum(theta=torch.abs(theta))

        # Compute dist.
        # # inv_dist_idx = torch.arange(0, N, device=device) # Bug. N may not be the maximum number of candidates.
        # inv_dist_idx = self.inv_dist_idx
        # dist = self.bf / ( inv_dist_idx + self.eps )
        dist = self.dist

        # Meshgrid.
        grid_dist, grid_phi, grid_theta = \
            torch_meshgrid( dist, phi, theta, indexing='ij' )

        show_sum(grid_dist=grid_dist, grid_phi=grid_phi, grid_theta=torch.abs(grid_theta))
        show_elements(torch.arange(20, device=grid_dist.device), grid_dist=grid_dist, grid_phi=grid_phi, grid_theta=grid_theta)

        # Intermediate values.
        sin_phi   = torch.sin( grid_phi )
        cos_phi   = torch.cos( grid_phi )
        sin_theta = torch.sin( grid_theta )
        cos_theta = torch.cos( grid_theta )

        ds =  grid_dist * sin_phi
        x  =  ds        * cos_theta
        y  = -grid_dist * cos_phi
        z  = -ds        * sin_theta

        # [ N, H, W ] -> [ 3, N, H, W ]
        t = torch.stack( (x, y, z), dim=0 )
        return FTensor(t, f0=self.frame_name)

class RayMaker(object):
    def __init__(self, camera_model, frame_name='rbf'):
        super().__init__()

        self.camera_model = camera_model
        self.frame_name   = frame_name

    def make_raw_rays(self):
        # Get the pixel coordinates of all the pixels of the camera model.
        # [3, N]
        pixel_coords = self.camera_model.pixel_coordinates(flatten=True)

        # Compute the rays and the mask.
        rays, valid_mask = self.camera_model.pixel_2_ray(pixel_coords)

        # Recover the shape of the rays.
        rays = rays.view( ( 3, *self.camera_model.shape ) ) # [3, H, W]
        valid_mask = valid_mask.view( (1, *self.camera_model.shape) ) # [1, H, W]

        return rays, valid_mask

    def make_rays_from_grid_distance(self, grid_distance):
        '''
        grid_distance must be a single-channel 2D tensor with the shape of [1, H, W]

        Returns:
        rays as a FTensor, [3, 1, H, W]
        valid_mask, [1, 1, H, W]
        '''

        if isinstance(grid_distance, np.ndarray):
            grid_distance = torch.from_numpy(grid_distance)

        # [3, H, W] and [1, H, W]
        rays, valid_mask = self.make_raw_rays()

        # Extend the dimension of rays.
        rays = rays.unsqueeze(1) # [3, H, W] -> [3, 1, H, W]

        # Extend the dimension of grid_distance.
        # [1, H, W] -> [1, 1, H, W]
        grid_distance = grid_distance.view( (1, *grid_distance.shape) )

        # Multiply.
        rays = rays * grid_distance

        # Duplicate the mask.
        # [1, H, W] -> [1, 1, H, W]
        valid_mask = valid_mask.unsqueeze(1)

        return FTensor(rays, f0=self.frame_name), valid_mask

    def make_rays_from_candidates(self, candidates):
        '''
        candidates is 1D array of shape [N, ]

        Returns:
        rays as a FTensor, [3, N, H, W]
        valid_mask, [1, N, H, W]
        '''

        if isinstance(candidates, np.ndarray):
            candidates = torch.from_numpy(candidates)

        # [3, H, W] and [1, H, W]
        rays, valid_mask = self.make_raw_rays()

        # Extend the dimension of rays.
        rays = rays.unsqueeze(1) # [3, H, W] -> [3, 1, H, W]

        # Extend the dimension of candidates.
        N = candidates.numel()
        candidates = candidates.view( (1, N, 1, 1) )

        # Multiply.
        rays = rays * candidates

        # Duplicate the mask.
        # [1, H, W] -> [1, N, H, W]
        valid_mask = valid_mask.unsqueeze(1).repeat(1, N, 1, 1)

        return FTensor(rays, f0=self.frame_name), valid_mask

    def make_rays(self, candidates=None, grid_distance=None):
        '''
        candidates and grid_distance cannot both be None, or both be not None.

        Returns:
        rays, and valid mask of rays.
        '''

        if candidates is not None and grid_distance is None:
            return self.make_rays_from_candidates(candidates)
        elif candidates is None and grid_distance is not None:
            return self.make_rays_from_grid_distance(grid_distance)
        else:
            raise Exception(
                f'candiates and grid_distance cannot both be None, '
                f'or both be not None. '
                f'type(candidates) = {type(candidates)}, '
                f'type(grid_distance) = {type(grid_distance)}. ')

class DoubleSphereSampleGridMaker(object):
    def __init__(self,
        params=[-0.203, 0.589, 232.0, 232.0, 611.5, 513.5 ],
        calib_shape=[1028, 1224], # Height, width
        ):
        super().__init__()

        # TODO: change the way for defining the intrinsics.
        # Maybe re-use the camera models?

        self.xi    = params[0]
        self.alpha = params[1]
        self.fx    = params[2]
        self.fy    = params[3]
        self.cx    = params[4]
        self.cy    = params[5]
        self.calib_shape = calib_shape

        if ( self.alpha <= 0.5 ):
            self.w1 = self.alpha / ( 1 - self.alpha )
        else:
            self.w1 = ( 1 - self.alpha ) / self.alpha

        self.w2 = ( self.w1 + self.xi ) / \
            np.sqrt( 2 * self.w1 * self.xi + self.xi**2 + 1 )

    def make_grid(self, points):
        '''
        NOTE: Differernt from warper.
        Compute the 2D sampling locations based on 3D points in the 
        field of view. Points is in [ B, 3, N, H, W ]. Where N is
        the nubmber of candidate stops. The output consists of a sampling
        location tensor and a mask. The sampling location tensor has a 
        dimension of [ B, N, H, W, 2 ] and the last dimension has a 
        range of [-1, 1]. The mask is [ B, N, H, W ] where True means valid.
        '''
        
        x, y, z = torch.split(points, 1, dim=1) # [ B, 1, N, H, W ]

        # Double-sphere model.
        x2 = x**2
        y2 = y**2
        z2 = z**2

        d1 = torch.sqrt( x2 + y2 + z2 )
        d2 = torch.sqrt( x2 + y2 + ( self.xi * d1 + z )**2 )

        t = self.alpha * d2 + ( 1 - self.alpha ) * ( self.xi * d1 + z )
        if isinstance(t, FTensor):
            t = t.tensor()

        ux = ( self.fx / t * x + self.cx ) / ( self.calib_shape[1] - 1 ) * 2 - 1
        uy = ( self.fy / t * y + self.cy ) / ( self.calib_shape[0] - 1 ) * 2 - 1

        # Reshape.
        B, _, N, H, W = points.shape
        ux = ux.view( ( B, N, H, W, 1) )
        uy = uy.view( ( B, N, H, W, 1) )

        mask = z > -self.w2 * d1
        mask = mask.view( ( B, N, H, W ) )

        return torch.cat( (ux, uy), dim=4 ), mask

class EquirectangularSampleGridMaker(object):

    def __init__(self):
        super().__init__()

    def make_grid(self, points):
        '''
        points: [B, 3, N, H, W] Tensor where each [i, :, j, k, l] index slice is a 3-element Tensor with the xyz coordinates of the ray corresponding 
                to the phi and theta coordinates of that pixel in the panoramic image. The height dimension indexes phi from [-π/2, π/2]
        '''
        
        x, y, z = torch.split(points, 1, dim=1) # [ B, 1, N, H, W ]

        #3D vector to longitude/latitude:
        x2 = x**2
        z2 = z**2
        #ypi = np.pi/2 + y

        xz_norm = torch.sqrt(x2 + z2)

        # TODO: this needs review.
        # It seems to be correct for the purpose of computing the grid sample locations.
        # However, it is not the original definitions for the logitude and latitude coordinates.
        longitude = -1 * torch.atan2( z, x )
        latitude = torch.atan2( y, xz_norm )

        B, _, N, H, W = points.shape

        # TODO: this needs review.
        u = ( longitude / np.pi )    # Needs to be in [-1, 1]
        v = ( 2 * latitude / np.pi ) # Needs to be in [-1, 1]

        u = u.view( ( B, N, H, W, 1) )
        v = v.view( ( B, N, H, W, 1) )

        return torch.cat( (u, v), dim=4 )

class CameraModelGridMaker(object):
    def __init__(self, camera_model):
        super().__init__()
        self.camera_model = camera_model
        
    def make_grid(self, points):
        '''
        NOTE: Differernt from warper.
        Compute the 2D sampling locations based on 3D points in the 
        field of view. Points is in [ B, 3, N, H, W ]. Where N is
        the nubmber of candidate stops. The output consists of a sampling
        location tensor and a mask. The sampling location tensor has a 
        dimension of [ B, N, H, W, 2 ] and the last dimension has a 
        range of [-1, 1]. The mask is [ B, N, H, W ] where True means valid.
        '''
        
        # Reshape points.
        B, C, N, H, W = points.shape
        points = points.view( ( B, C, N*H*W ) )
        
        show_sum(points=points)
        
        # Compute the pixel coordinates.
        if isinstance(points, FTensor):
            points = points.tensor()
        pixel_coor, valid_mask = self.camera_model.point_3d_2_pixel(points, normalized=True)

        show_sum(pixel_coor=pixel_coor, valid_mask=valid_mask.to(torch.int))

        # Reshape the results.
        pixel_coor = pixel_coor.view(    ( B, 2, N, H, W ) )
        pixel_coor = pixel_coor.permute( ( 0, 2, 3, 4, 1 ) )
        
        valid_mask = valid_mask.view( ( B, N, H, W, 1 ) )

        return pixel_coor, valid_mask
    
    def __str__(self):
        return \
f'''CameraModelGridMaker: {{
    camera_model: {self.camera_model} }}'''
        
CAMERA_MODEL_GRID_MAKER_MAP = {
    'DoubleSphere': DoubleSphereSampleGridMaker,
    'Equirectangular': EquirectangularSampleGridMaker,
    'CameraModel': CameraModelGridMaker,
}

def transform_3D_points_torch(T, points):
    '''
    NOTE: Different from warper.
    T (Tensor): [ B, 4, 4 ].
    points (Tensor): [ B, 3, N, H, W ].
    '''

    # Dimensions.
    B, _, N, H, W = points.shape

    # Reshape.
    points = points.view( ( B, 3, N * H * W ) )

    # Get the R and t.
    R = T[:, :3, :3]
    t = T[:, :3,  3].unsqueeze(2)

    # Transform.
    points = torch.matmul( R, points ) + t

    # Reshape.
    points = points.view( ( B, 3, N, H, W ) )

    return points

